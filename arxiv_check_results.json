{
  "published": [
    {
      "key": "kirk2023understanding",
      "original_title": "Understanding the effects of rlhf on llm generalisation and diversity",
      "venue": "ICLR",
      "year": "2024",
      "dblp_title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity.",
      "dblp_key": "conf/iclr/KirkMNLHGR24"
    },
    {
      "key": "chu2025sft",
      "original_title": "Sft memorizes, rl generalizes: A comparative study of foundation model post-training",
      "venue": "ICML",
      "year": "2025",
      "dblp_title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training.",
      "dblp_key": "conf/icml/ChuZYTXSLL025"
    },
    {
      "key": "park2024disentangling",
      "original_title": "Disentangling length from quality in direct preference optimization",
      "venue": "ACL",
      "year": "2024",
      "dblp_title": "Disentangling Length from Quality in Direct Preference Optimization.",
      "dblp_key": "conf/acl/ParkREF24"
    },
    {
      "key": "muennighoff2024olmoe",
      "original_title": "Olmoe: Open mixture-of-experts language models",
      "venue": "ICLR",
      "year": "2025",
      "dblp_title": "OLMoE: Open Mixture-of-Experts Language Models.",
      "dblp_key": "conf/iclr/MuennighoffSGLM25"
    },
    {
      "key": "kaufmann2023survey",
      "original_title": "A survey of reinforcement learning from human feedback",
      "venue": "Trans. Mach. Learn. Res.",
      "year": "2025",
      "dblp_title": "A Survey of Reinforcement Learning from Human Feedback.",
      "dblp_key": "journals/tmlr/KaufmannWBH25"
    },
    {
      "key": "lightman2023let",
      "original_title": "Let's verify step by step",
      "venue": "ICLR",
      "year": "2024",
      "dblp_title": "Let&apos;s Verify Step by Step.",
      "dblp_key": "conf/iclr/LightmanKBEBLLS24"
    },
    {
      "key": "ramamurthy2022reinforcement",
      "original_title": "Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization",
      "venue": "ICLR",
      "year": "2023",
      "dblp_title": "Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization.",
      "dblp_key": "conf/iclr/RamamurthyABHSB23"
    },
    {
      "key": "casper2023open",
      "original_title": "Open problems and fundamental limitations of reinforcement learning from human feedback",
      "venue": "Trans. Mach. Learn. Res.",
      "year": "2023",
      "dblp_title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback.",
      "dblp_key": "journals/tmlr/CasperDSGSRFKLF23"
    },
    {
      "key": "kumar2024training",
      "original_title": "Training language models to self-correct via reinforcement learning",
      "venue": "ICLR",
      "year": "2025",
      "dblp_title": "Training Language Models to Self-Correct via Reinforcement Learning.",
      "dblp_key": "conf/iclr/KumarZASCSBIBRZ25"
    },
    {
      "key": "singh2023beyond",
      "original_title": "Beyond human data: Scaling self-training for problem-solving with language models",
      "venue": "Trans. Mach. Learn. Res.",
      "year": "2024",
      "dblp_title": "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models.",
      "dblp_key": "journals/tmlr/SinghCAAPGLH0XP24"
    },
    {
      "key": "conitzer2024social",
      "original_title": "Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback",
      "venue": "ICML",
      "year": "2024",
      "dblp_title": "Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback.",
      "dblp_key": "conf/icml/ConitzerFHHJ0MP24"
    },
    {
      "key": "poddar2024personalizing",
      "original_title": "Personalizing reinforcement learning from human feedback with variational preference learning",
      "venue": "NeurIPS",
      "year": "2024",
      "dblp_title": "Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning.",
      "dblp_key": "conf/nips/PoddarWIGJ24"
    },
    {
      "key": "pitis2023consistent",
      "original_title": "Consistent Aggregation of Objectives with Diverse Time Preferences Requires Non-Markovian Rewards",
      "venue": "NeurIPS",
      "year": "2023",
      "dblp_title": "Consistent Aggregation of Objectives with Diverse Time Preferences Requires Non-Markovian Rewards.",
      "dblp_key": "conf/nips/Pitis23"
    },
    {
      "key": "zhang2024lists",
      "original_title": "From lists to emojis: How format bias affects model alignment",
      "venue": "ACL",
      "year": "2025",
      "dblp_title": "From Lists to Emojis: How Format Bias Affects Model Alignment.",
      "dblp_key": "conf/acl/Zhang0C0H025"
    },
    {
      "key": "kumar2025detecting",
      "original_title": "Detecting Prefix Bias in LLM-based Reward Models",
      "venue": "FAccT",
      "year": "2025",
      "dblp_title": "Detecting Prefix Bias in LLM-based Reward Models.",
      "dblp_key": "conf/fat/KumarHMCI25"
    },
    {
      "key": "wang2024helpsteer2p",
      "original_title": "HelpSteer2-Preference: Complementing Ratings with Preferences",
      "venue": "ICLR",
      "year": "2025",
      "dblp_title": "HelpSteer2-Preference: Complementing Ratings with Preferences.",
      "dblp_key": "conf/iclr/WangBDESZKD25"
    },
    {
      "key": "chiang2024chatbot",
      "original_title": "Chatbot arena: An open platform for evaluating llms by human preference",
      "venue": "ICML",
      "year": "2024",
      "dblp_title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference.",
      "dblp_key": "conf/icml/ChiangZ0ALLZ0JG24"
    },
    {
      "key": "lambert2024rewardbench",
      "original_title": "Rewardbench: Evaluating reward models for language modeling",
      "venue": "NAACL",
      "year": "2025",
      "dblp_title": "RewardBench: Evaluating Reward Models for Language Modeling.",
      "dblp_key": "conf/naacl/LambertPMMLCDKZCSH25"
    },
    {
      "key": "zhou2024rmb",
      "original_title": "RMB: Comprehensively Benchmarking Reward Models in LLM Alignment",
      "venue": "ICLR",
      "year": "2025",
      "dblp_title": "RMB: Comprehensively benchmarking reward models in LLM alignment.",
      "dblp_key": "conf/iclr/ZhouZWXDBSXFMZG25"
    },
    {
      "key": "liu2024rm",
      "original_title": "RM-bench: Benchmarking reward models of language models with subtlety and style",
      "venue": "ICLR",
      "year": "2025",
      "dblp_title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style.",
      "dblp_key": "conf/iclr/Liu0M00L25"
    },
    {
      "key": "gureja2024m",
      "original_title": "M-RewardBench: Evaluating Reward Models in Multilingual Settings",
      "venue": "ACL",
      "year": "2025",
      "dblp_title": "M-RewardBench: Evaluating Reward Models in Multilingual Settings.",
      "dblp_key": "conf/acl/GurejaMIMSW0RHF25"
    },
    {
      "key": "song2025prmbench",
      "original_title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models",
      "venue": "ACL",
      "year": "2025",
      "dblp_title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models.",
      "dblp_key": "conf/acl/SongSQZ025"
    },
    {
      "key": "wen2024rethinking",
      "original_title": "Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?",
      "venue": "ICLR",
      "year": "2025",
      "dblp_title": "Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?",
      "dblp_key": "conf/iclr/WenL0LXLHHZ025"
    },
    {
      "key": "jin2024rag",
      "original_title": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment",
      "venue": "ACL",
      "year": "2025",
      "dblp_title": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment.",
      "dblp_key": "conf/acl/JinYMC0XLJ0025"
    },
    {
      "key": "frick2024evaluate",
      "original_title": "How to Evaluate Reward Models for RLHF",
      "venue": "ICLR",
      "year": "2025",
      "dblp_title": "How to Evaluate Reward Models for RLHF.",
      "dblp_key": "conf/iclr/FrickLCCAJZGS25"
    },
    {
      "key": "wang2024interpretable",
      "original_title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
      "venue": "EMNLP",
      "year": "2024",
      "dblp_title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts.",
      "dblp_key": "conf/emnlp/00030X0024"
    },
    {
      "key": "zhang2024generative",
      "original_title": "Generative verifiers: Reward modeling as next-token prediction",
      "venue": "ICLR",
      "year": "2025",
      "dblp_title": "Generative Verifiers: Reward Modeling as Next-Token Prediction.",
      "dblp_key": "conf/iclr/ZhangHBKKA25"
    },
    {
      "key": "park2024offsetbias",
      "original_title": "Offsetbias: Leveraging debiased data for tuning evaluators",
      "venue": "EMNLP",
      "year": "2024",
      "dblp_title": "OffsetBias: Leveraging Debiased Data for Tuning Evaluators.",
      "dblp_key": "conf/emnlp/ParkJRKC24"
    },
    {
      "key": "liu2024acemath",
      "original_title": "AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling",
      "venue": "ACL",
      "year": "2025",
      "dblp_title": "AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling.",
      "dblp_key": "conf/acl/LiuCSCP25"
    },
    {
      "key": "zheng2024processbench",
      "original_title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
      "venue": "ACL",
      "year": "2025",
      "dblp_title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning.",
      "dblp_key": "conf/acl/ZhengZZLLYLZL25"
    },
    {
      "key": "lin2024wildbench",
      "original_title": "WILDBENCH: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
      "venue": "ICLR",
      "year": "2025",
      "dblp_title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild.",
      "dblp_key": "conf/iclr/LinDCRPD0025"
    },
    {
      "key": "li2024crowdsourced",
      "original_title": "From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline",
      "venue": "ICML",
      "year": "2025",
      "dblp_title": "From Crowdsourced Data to High-quality Benchmarks: Arena-Hard and Benchbuilder Pipeline.",
      "dblp_key": "conf/icml/LiCFD0ZGS25"
    },
    {
      "key": "jaques2020human",
      "original_title": "Human-centric dialog training via offline reinforcement learning",
      "venue": "EMNLP",
      "year": "2020",
      "dblp_title": "Human-centric dialog training via offline reinforcement learning.",
      "dblp_key": "conf/emnlp/JaquesSGFLJGP20"
    },
    {
      "key": "pang2024iterative",
      "original_title": "Iterative reasoning preference optimization",
      "venue": "NeurIPS",
      "year": "2024",
      "dblp_title": "Iterative Reasoning Preference Optimization.",
      "dblp_key": "conf/nips/PangYHCSW24"
    },
    {
      "key": "gao2024rebel",
      "original_title": "Rebel: Reinforcement learning via regressing relative rewards",
      "venue": "NeurIPS",
      "year": "2024",
      "dblp_title": "REBEL: Reinforcement Learning via Regressing Relative Rewards.",
      "dblp_key": "conf/nips/GaoCZOSBJBL024"
    },
    {
      "key": "dong2023raft",
      "original_title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment",
      "venue": "Trans. Mach. Learn. Res.",
      "year": "2023",
      "dblp_title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment.",
      "dblp_key": "journals/tmlr/Dong0GZCPDZS023"
    },
    {
      "key": "liu2023statistical",
      "original_title": "Statistical Rejection Sampling Improves Preference Optimization",
      "venue": "ICLR",
      "year": "2024",
      "dblp_title": "Statistical Rejection Sampling Improves Preference Optimization.",
      "dblp_key": "conf/iclr/0002ZJKSLL24"
    },
    {
      "key": "ivison2024unpacking",
      "original_title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback",
      "venue": "NeurIPS",
      "year": "2024",
      "dblp_title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback.",
      "dblp_key": "conf/nips/IvisonW0WP0S0H24"
    },
    {
      "key": "baheti2023leftover",
      "original_title": "Leftover lunch: advantage-based offline reinforcement learning for language models",
      "venue": "ICLR",
      "year": "2024",
      "dblp_title": "Leftover Lunch: Advantage-based Offline Reinforcement Learning for Language Models.",
      "dblp_key": "conf/iclr/BahetiLB0SR24"
    },
    {
      "key": "flet2024contrastive",
      "original_title": "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion",
      "venue": "EMNLP",
      "year": "2024",
      "dblp_title": "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion.",
      "dblp_key": "conf/emnlp/Flet-BerliacGSC24"
    },
    {
      "key": "tomar2020mirror",
      "original_title": "Mirror descent policy optimization",
      "venue": "ICLR",
      "year": "2022",
      "dblp_title": "Mirror Descent Policy Optimization.",
      "dblp_key": "conf/iclr/TomarSEG22"
    },
    {
      "key": "noukhovitch2024asynchronous",
      "original_title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models",
      "venue": "ICLR",
      "year": "2025",
      "dblp_title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models.",
      "dblp_key": "conf/iclr/NoukhovitchHXHA25"
    },
    {
      "key": "ahmadian2024back",
      "original_title": "Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms",
      "venue": "ACL",
      "year": "2024",
      "dblp_title": "Back to Basics: Revisiting REINFORCE-Style Optimization for Learning from Human Feedback in LLMs.",
      "dblp_key": "conf/acl/AhmadianCGFKPUH24"
    },
    {
      "key": "xu2024dpo",
      "original_title": "Is dpo superior to ppo for llm alignment? a comprehensive study",
      "venue": "ICML",
      "year": "2024",
      "dblp_title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study.",
      "dblp_key": "conf/icml/XuFGYLMW0024"
    },
    {
      "key": "amini2024direct",
      "original_title": "Direct preference optimization with an offset",
      "venue": "ACL",
      "year": "2024",
      "dblp_title": "Direct Preference Optimization with an Offset.",
      "dblp_key": "conf/acl/AminiVC24"
    },
    {
      "key": "zhao2024rainbowpo",
      "original_title": "Rainbowpo: A unified framework for combining improvements in preference optimization",
      "venue": "ICLR",
      "year": "2025",
      "dblp_title": "RainbowPO: A Unified Framework for Combining Improvements in Preference Optimization.",
      "dblp_key": "conf/iclr/ZhaoWDZYTS25"
    },
    {
      "key": "jung2024binary",
      "original_title": "Binary classifier optimization for large language model alignment",
      "venue": "ACL",
      "year": "2025",
      "dblp_title": "Binary Classifier Optimization for Large Language Model Alignment.",
      "dblp_key": "conf/acl/JungHNO25"
    },
    {
      "key": "razin2024unintentional",
      "original_title": "Unintentional unalignment: Likelihood displacement in direct preference optimization",
      "venue": "ICLR",
      "year": "2025",
      "dblp_title": "Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization.",
      "dblp_key": "conf/iclr/RazinMB0AH25"
    },
    {
      "key": "ren2024learning",
      "original_title": "Learning dynamics of llm finetuning",
      "venue": "ICLR",
      "year": "2025",
      "dblp_title": "Learning Dynamics of LLM Finetuning.",
      "dblp_key": "conf/iclr/RenS25"
    },
    {
      "key": "xiao2024cal",
      "original_title": "Cal-dpo: Calibrated direct preference optimization for language model alignment",
      "venue": "NeurIPS",
      "year": "2024",
      "dblp_title": "Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment.",
      "dblp_key": "conf/nips/XiaoYZLH24"
    },
    {
      "key": "gupta2025alphapo",
      "original_title": "AlphaPO--Reward shape matters for LLM alignment",
      "venue": "ICML",
      "year": "2025",
      "dblp_title": "AlphaPO: Reward Shape Matters for LLM Alignment.",
      "dblp_key": "conf/icml/GuptaTSZHSGLKZA25"
    },
    {
      "key": "tajwar2024preference",
      "original_title": "Preference fine-tuning of llms should leverage suboptimal, on-policy data",
      "venue": "ICML",
      "year": "2024",
      "dblp_title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data.",
      "dblp_key": "conf/icml/TajwarSSR0XEFK24"
    },
    {
      "key": "sharma2024critical",
      "original_title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models",
      "venue": "NeurIPS",
      "year": "2024",
      "dblp_title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models.",
      "dblp_key": "conf/nips/SharmaKMFAK24"
    },
    {
      "key": "yuan2025selfrewardinglanguagemodels",
      "original_title": "Self-Rewarding Language Models",
      "venue": "ACL",
      "year": "2025",
      "dblp_title": "Process-based Self-Rewarding Language Models.",
      "dblp_key": "conf/acl/ZhangLZLLHG25"
    },
    {
      "key": "zhao2025sample",
      "original_title": "Sample, scrutinize and scale: Effective inference-time search by scaling verification",
      "venue": "ICML",
      "year": "2025",
      "dblp_title": "Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification.",
      "dblp_key": "conf/icml/0003AG25"
    },
    {
      "key": "wang2023large",
      "original_title": "Large language models are not fair evaluators",
      "venue": "ACL",
      "year": "2024",
      "dblp_title": "Large Language Models are not Fair Evaluators.",
      "dblp_key": "conf/acl/WangLCCZLCKLLS24"
    },
    {
      "key": "xu2025rlthf",
      "original_title": "RLTHF: Targeted Human Feedback for LLM Alignment",
      "venue": "ICML",
      "year": "2025",
      "dblp_title": "RLTHF: Targeted Human Feedback for LLM Alignment.",
      "dblp_key": "conf/icml/XuCKASLC25"
    },
    {
      "key": "kim2024prometheus",
      "original_title": "Prometheus 2: An open source language model specialized in evaluating other language models",
      "venue": "EMNLP",
      "year": "2024",
      "dblp_title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models.",
      "dblp_key": "conf/emnlp/KimSLLSWNL0S24"
    },
    {
      "key": "ke2023critiquellm",
      "original_title": "CritiqueLLM: Towards an informative critique generation model for evaluation of large language model generation",
      "venue": "ACL",
      "year": "2024",
      "dblp_title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation.",
      "dblp_key": "conf/acl/KeWFLLCWZDWTH24"
    },
    {
      "key": "li2023generative",
      "original_title": "Generative Judge for Evaluating Alignment",
      "venue": "ICLR",
      "year": "2024",
      "dblp_title": "Generative Judge for Evaluating Alignment.",
      "dblp_key": "conf/iclr/LiSYF0024"
    },
    {
      "key": "sheng2024hybridflow",
      "original_title": "HybridFlow: A Flexible and Efficient RLHF Framework",
      "venue": "EuroSys",
      "year": "2025",
      "dblp_title": "HybridFlow: A Flexible and Efficient RLHF Framework.",
      "dblp_key": "conf/eurosys/ShengZYWZZPL025"
    },
    {
      "key": "gehring2024rlefgroundingcodellms",
      "original_title": "RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning",
      "venue": "ICML",
      "year": "2025",
      "dblp_title": "RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning.",
      "dblp_key": "conf/icml/GehringZCMCS25"
    },
    {
      "key": "amit2024models",
      "original_title": "Models that prove their own correctness",
      "venue": "Electron. Colloquium Comput. Complex.",
      "year": "2024",
      "dblp_title": "Models That Prove Their Own Correctness.",
      "dblp_key": "journals/eccc/AmitPRG24"
    },
    {
      "key": "mirhoseini2020chip",
      "original_title": "Chip placement with deep reinforcement learning",
      "venue": "DATE",
      "year": "2023",
      "dblp_title": "DeepTH: Chip Placement with Deep Reinforcement Learning Using a Three-Head Policy Network.",
      "dblp_key": "conf/date/ZhaoYSTX23"
    },
    {
      "key": "cusumano2025robust",
      "original_title": "Robust autonomy emerges from self-play",
      "venue": "ICML",
      "year": "2025",
      "dblp_title": "Robust Autonomy Emerges from Self-Play.",
      "dblp_key": "conf/icml/Cusumano-Towner25"
    },
    {
      "key": "reed2015neural",
      "original_title": "Neural programmer-interpreters",
      "venue": "ICLR",
      "year": "2016",
      "dblp_title": "Neural Programmer-Interpreters.",
      "dblp_key": "journals/corr/ReedF15"
    },
    {
      "key": "schick2023toolformerlanguagemodelsteach",
      "original_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
      "venue": "NeurIPS",
      "year": "2023",
      "dblp_title": "Toolformer: Language Models Can Teach Themselves to Use Tools.",
      "dblp_key": "conf/nips/SchickDDRLHZCS23"
    },
    {
      "key": "patil2023gorilla",
      "original_title": "Gorilla: Large Language Model Connected with Massive APIs",
      "venue": "NeurIPS",
      "year": "2024",
      "dblp_title": "Gorilla: Large Language Model Connected with Massive APIs.",
      "dblp_key": "conf/nips/PatilZ0G24"
    },
    {
      "key": "qin2023toollm",
      "original_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
      "venue": "ICLR",
      "year": "2024",
      "dblp_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs.",
      "dblp_key": "conf/iclr/QinLYZYLLCTQZHT24"
    },
    {
      "key": "li2024mmedagent",
      "original_title": "Mmedagent: Learning to use medical tools with multi-modal agent",
      "venue": "EMNLP",
      "year": "2024",
      "dblp_title": "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent.",
      "dblp_key": "conf/emnlp/LiYPLJDXLDLW24"
    },
    {
      "key": "wang2022self",
      "original_title": "Self-instruct: Aligning language models with self-generated instructions",
      "venue": "ACL",
      "year": "2023",
      "dblp_title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions.",
      "dblp_key": "conf/acl/WangKMLSKH23"
    },
    {
      "key": "li2024superfiltering",
      "original_title": "Superfiltering: Weak-to-strong data filtering for fast instruction-tuning",
      "venue": "ACL",
      "year": "2024",
      "dblp_title": "Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning.",
      "dblp_key": "conf/acl/LiZHLZWCZ24"
    },
    {
      "key": "huang2025math",
      "original_title": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations",
      "venue": "ICML",
      "year": "2025",
      "dblp_title": "MATH-Perturb: Benchmarking LLMs&apos; Math Reasoning Abilities against Hard Perturbations.",
      "dblp_key": "conf/icml/HuangGLJ0LGC0WW25"
    },
    {
      "key": "hendrycks2020measuring",
      "original_title": "Measuring massive multitask language understanding",
      "venue": "NAACL",
      "year": "2025",
      "dblp_title": "KMMLU: Measuring Massive Multitask Language Understanding in Korean.",
      "dblp_key": "conf/naacl/SonLKKMCPYB25"
    },
    {
      "key": "lin2021truthfulqa",
      "original_title": "Truthfulqa: Measuring how models mimic human falsehoods",
      "venue": "ACL",
      "year": "2022",
      "dblp_title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods.",
      "dblp_key": "conf/acl/LinHE22"
    },
    {
      "key": "suzgun2022challenging",
      "original_title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
      "venue": "ACL",
      "year": "2023",
      "dblp_title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them.",
      "dblp_key": "conf/acl/SuzgunSSGTCCLCZ23"
    },
    {
      "key": "dua2019drop",
      "original_title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "venue": "NAACL-HLT",
      "year": "2019",
      "dblp_title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs.",
      "dblp_key": "conf/naacl/DuaWDSS019"
    },
    {
      "key": "gu2024olmes",
      "original_title": "OLMES: A Standard for Language Model Evaluations",
      "venue": "NAACL",
      "year": "2025",
      "dblp_title": "OLMES: A Standard for Language Model Evaluations.",
      "dblp_key": "conf/naacl/GuTKHDH25"
    },
    {
      "key": "liang2023helm",
      "original_title": "Holistic Evaluation of Language Models",
      "venue": "ACL",
      "year": "2025",
      "dblp_title": "SEA-HELM: Southeast Asian Holistic Evaluation of Language Models.",
      "dblp_key": "conf/acl/SusantoHMNYLRL025"
    },
    {
      "key": "yu2023metamath",
      "original_title": "Metamath: Bootstrap your own mathematical questions for large language models",
      "venue": "ICLR",
      "year": "2024",
      "dblp_title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models.",
      "dblp_key": "conf/iclr/YuJSYLZKLWL24"
    },
    {
      "key": "coste2023reward",
      "original_title": "Reward model ensembles help mitigate overoptimization",
      "venue": "ICLR",
      "year": "2024",
      "dblp_title": "Reward Model Ensembles Help Mitigate Overoptimization.",
      "dblp_key": "conf/iclr/CosteAK024"
    },
    {
      "key": "moskovitz2023confronting",
      "original_title": "Confronting reward model overoptimization with constrained RLHF",
      "venue": "ICLR",
      "year": "2024",
      "dblp_title": "Confronting Reward Model Overoptimization with Constrained RLHF.",
      "dblp_key": "conf/iclr/MoskovitzSSSSDM24"
    },
    {
      "key": "rottger2023xstest",
      "original_title": "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
      "venue": "NAACL-HLT",
      "year": "2024",
      "dblp_title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models.",
      "dblp_key": "conf/naacl/RottgerKVA0H24"
    },
    {
      "key": "han2024wildguard",
      "original_title": "Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms",
      "venue": "NeurIPS",
      "year": "2024",
      "dblp_title": "WildGuard: Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs.",
      "dblp_key": "conf/nips/HanREJL00D24"
    },
    {
      "key": "wang2023openchat",
      "original_title": "Openchat: Advancing open-source language models with mixed-quality data",
      "venue": "ICLR",
      "year": "2024",
      "dblp_title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data.",
      "dblp_key": "conf/iclr/WangCZLSL24"
    }
  ],
  "arxiv_only": [
    {
      "key": "ivison2023camels",
      "title": "Camels in a changing climate: Enhancing lm adaptation with tulu 2"
    },
    {
      "key": "adler2024nemotron",
      "title": "Nemotron-4 340B Technical Report"
    },
    {
      "key": "nakano2021webgpt",
      "title": "Webgpt: Browser-assisted question-answering with human feedback"
    },
    {
      "key": "askell2021general",
      "title": "A general language assistant as a laboratory for alignment"
    },
    {
      "key": "bai2022training",
      "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
    },
    {
      "key": "bai2022constitutional",
      "title": "Constitutional ai: Harmlessness from ai feedback"
    },
    {
      "key": "dubey2024llama",
      "title": "The llama 3 herd of models"
    },
    {
      "key": "guo2025deepseek",
      "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning"
    },
    {
      "key": "singhal2023long",
      "title": "A long way to go: Investigating length correlations in rlhf"
    },
    {
      "key": "leike2018scalable",
      "title": "Scalable agent alignment via reward modeling: a research direction"
    },
    {
      "key": "ziegler2019fine",
      "title": "Fine-tuning language models from human preferences"
    },
    {
      "key": "wu2021recursively",
      "title": "Recursively summarizing books with human feedback"
    },
    {
      "key": "ganguli2022red",
      "title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned"
    },
    {
      "key": "glaese2022improving",
      "title": "Improving alignment of dialogue agents via targeted human judgements"
    },
    {
      "key": "menick2022teaching",
      "title": "Teaching language models to support answers with verified quotes"
    },
    {
      "key": "touvron2023llama",
      "title": "Llama 2: Open foundation and fine-tuned chat models"
    },
    {
      "key": "team2024gemma",
      "title": "Gemma 2: Improving open language models at a practical size"
    },
    {
      "key": "hinton2015distilling",
      "title": "Distilling the knowledge in a neural network"
    },
    {
      "key": "cohere2025command",
      "title": "Command A: An Enterprise-Ready Large Language Model"
    },
    {
      "key": "alrashed2024smoltulu",
      "title": "SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs"
    },
    {
      "key": "olmo20242",
      "title": "2 OLMo 2 Furious"
    },
    {
      "key": "li2022branch",
      "title": "Branch-train-merge: Embarrassingly parallel training of expert language models"
    },
    {
      "key": "yang2025qwen3",
      "title": "Qwen3 technical report"
    },
    {
      "key": "xia2025mimo",
      "title": "MiMo: Unlocking the Reasoning Potential of Language Model--From Pretraining to Posttraining"
    },
    {
      "key": "lambert2023entangled",
      "title": "Entangled preferences: The history and risks of reinforcement learning and human feedback"
    },
    {
      "key": "mishra2023ai",
      "title": "Ai alignment and social choice: Fundamental limitations and policy implications"
    },
    {
      "key": "mnih2013playing",
      "title": "Playing atari with deep reinforcement learning"
    },
    {
      "key": "gilbert2022choices",
      "title": "Choices, Risks, and Reward Reports: Charting Public Policy for Reinforcement Learning Systems"
    },
    {
      "key": "fickinger2020multi",
      "title": "Multi-principal assistance games"
    },
    {
      "key": "bharadwaj2025flatteryflufffogdiagnosing",
      "title": "Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases in Preference Models"
    },
    {
      "key": "malik2025rewardbench",
      "title": "RewardBench 2: Advancing Reward Model Evaluation"
    },
    {
      "key": "ethayarajh2024kto",
      "title": "Kto: Model alignment as prospect theoretic optimization"
    },
    {
      "key": "zhou2023instructionfollowingevaluationlargelanguage",
      "title": "Instruction-Following Evaluation for Large Language Models"
    },
    {
      "key": "chen2024mj",
      "title": "MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?"
    },
    {
      "key": "wu2025rewordbench",
      "title": "reWordBench: Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs"
    },
    {
      "key": "yasunaga2025multimodal",
      "title": "Multimodal rewardbench: Holistic evaluation of reward models for vision language models"
    },
    {
      "key": "li2024vlrewardbench",
      "title": "VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models"
    },
    {
      "key": "ruan2025vlrmbench",
      "title": "Vlrmbench: A comprehensive and challenging benchmark for vision-language reward models"
    },
    {
      "key": "wang2025visualprm",
      "title": "VisualPRM: An Effective Process Reward Model for Multimodal Reasoning"
    },
    {
      "key": "tu2025vilbench",
      "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling"
    },
    {
      "key": "kim2024evaluating",
      "title": "Evaluating robustness of reward models for mathematical reasoning"
    },
    {
      "key": "wang2024helpsteer2",
      "title": "HelpSteer2: Open-source dataset for training top-performing reward models"
    },
    {
      "key": "ankner2024critique",
      "title": "Critique-out-loud reward models"
    },
    {
      "key": "lin2025cuarewardbench",
      "title": "CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent"
    },
    {
      "key": "lyu2025exploring",
      "title": "Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning"
    },
    {
      "key": "dubois2024length",
      "title": "Length-controlled alpacaeval: A simple way to debias automatic evaluators"
    },
    {
      "key": "cobbe2021gsm8k",
      "title": "Training Verifiers to Solve Math Word Problems"
    },
    {
      "key": "wallace2024instruction",
      "title": "The instruction hierarchy: Training llms to prioritize privileged instructions"
    },
    {
      "key": "schulman2017proximal",
      "title": "Proximal policy optimization algorithms"
    },
    {
      "key": "berner2019dota",
      "title": "Dota 2 with large scale deep reinforcement learning"
    },
    {
      "key": "yuan2025vapo",
      "title": "VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks"
    },
    {
      "key": "yuan2025s",
      "title": "What's Behind PPO's Collapse in Long-CoT? Value Optimization Holds the Secret"
    },
    {
      "key": "liu2025understanding",
      "title": "Understanding R1-Zero-Like Training: A Critical Perspective"
    },
    {
      "key": "team2025kimi",
      "title": "Kimi k1. 5: Scaling reinforcement learning with llms"
    },
    {
      "key": "wu2023pairwise",
      "title": "Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment"
    },
    {
      "key": "gunter2024apple",
      "title": "Apple intelligence foundation language models"
    },
    {
      "key": "zhang2025improving",
      "title": "Improving LLM General Preference Alignment via Optimistic Online Mirror Descent"
    },
    {
      "key": "wu2025llamarl",
      "title": "LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin"
    },
    {
      "key": "fu2025areal",
      "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning"
    },
    {
      "key": "roux2025tapered",
      "title": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs"
    },
    {
      "key": "primeintellectteam2025intellect2reasoningmodeltrained",
      "title": "INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized Reinforcement Learning"
    },
    {
      "key": "shao2024deepseekmath",
      "title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models"
    },
    {
      "key": "liu2024deepseek",
      "title": "Deepseek-v3 technical report"
    },
    {
      "key": "teamolmo2025olmo3",
      "title": "Olmo 3"
    },
    {
      "key": "zhao2023slic",
      "title": "Slic-hf: Sequence likelihood calibration with human feedback"
    },
    {
      "key": "singhal2024d2po",
      "title": "D2po: Discriminator-guided dpo with response evaluation models"
    },
    {
      "key": "guo2024direct",
      "title": "Direct language model alignment from online ai feedback"
    },
    {
      "key": "rosset2024direct",
      "title": "Direct nash optimization: Teaching language models to self-improve with general preferences"
    },
    {
      "key": "gorbatovski2025differences",
      "title": "The Differences Between Direct Alignment Algorithms are a Blur"
    },
    {
      "key": "wang2025helpsteer3",
      "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages"
    },
    {
      "key": "liu2025inference",
      "title": "Inference-Time Scaling for Generalist Reward Modeling"
    },
    {
      "key": "castricato2024suppressing",
      "title": "Suppressing Pink Elephants with Direct Principle Feedback"
    },
    {
      "key": "bercovich2025llamanemotron",
      "title": "Llama\u2011Nemotron: Efficient Reasoning Models"
    },
    {
      "key": "kalra2025verdict",
      "title": "Verdict: A Library for Scaling Judge-Time Compute"
    },
    {
      "key": "pace2024west",
      "title": "West-of-n: Synthetic preference generation for improved reward modeling"
    },
    {
      "key": "wu2024meta",
      "title": "Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge"
    },
    {
      "key": "brown2024large",
      "title": "Large language monkeys: Scaling inference compute with repeated sampling"
    },
    {
      "key": "guan2024deliberative",
      "title": "Deliberative alignment: Reasoning enables safer language models"
    },
    {
      "key": "lambert2024self",
      "title": "Self-directed synthetic dialogues and revisions technical report"
    },
    {
      "key": "wang2023shepherd",
      "title": "Shepherd: A critic for language model generation"
    },
    {
      "key": "aggarwal2025l1",
      "title": "L1: Controlling how long a reasoning model thinks with reinforcement learning"
    },
    {
      "key": "hu2025openreasonerzero",
      "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model"
    },
    {
      "key": "abdin2025phi4",
      "title": "Phi-4-Reasoning Technical Report"
    },
    {
      "key": "he2025skyworkor1",
      "title": "Skywork Open Reasoner\u00a01 Technical Report"
    },
    {
      "key": "guha2025openthoughts",
      "title": "OpenThoughts: Data Recipes for Reasoning Models"
    },
    {
      "key": "liu2025hunyuan",
      "title": "Hunyuan\u2011TurboS: Advancing Large Language Models through Mamba\u2011Transformer Synergy and Adaptive Chain\u2011of\u2011Thought"
    },
    {
      "key": "minimax2025minimaxm1scalingtesttimecompute",
      "title": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention"
    },
    {
      "key": "zeng2025glm45",
      "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models"
    },
    {
      "key": "coreteam2025mimovltechnicalreport",
      "title": "MiMo-VL Technical Report"
    },
    {
      "key": "kimiteam2025kimik2",
      "title": "Kimi K2: Open Agentic Intelligence"
    },
    {
      "key": "nvidia2025nemotronnano2",
      "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model"
    },
    {
      "key": "llm3602025k2think",
      "title": "K2-Think: A Parameter-Efficient Reasoning System"
    },
    {
      "key": "ringteam2025everystepevolves",
      "title": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model"
    },
    {
      "key": "chen2024more",
      "title": "Are more llm calls all you need? towards scaling laws of compound inference systems"
    },
    {
      "key": "muennighoff2025s1",
      "title": "s1: Simple test-time scaling"
    },
    {
      "key": "hu2024openrlhf",
      "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework"
    },
    {
      "key": "VinePPO",
      "title": "VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment"
    },
    {
      "key": "wang2025ragenunderstandingselfevolutionllm",
      "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning"
    },
    {
      "key": "khatri2025art",
      "title": "The art of scaling reinforcement learning compute for llms"
    },
    {
      "key": "parisi2022talm",
      "title": "Talm: Tool augmented language models"
    },
    {
      "key": "gerstgrasser2024model",
      "title": "Is model collapse inevitable? breaking the curse of recursion by accumulating real and synthetic data"
    },
    {
      "key": "wu2025reasoning",
      "title": "Reasoning or memorization? unreliable results of reinforcement learning due to data contamination"
    },
    {
      "key": "singh2024evaluation",
      "title": "Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?"
    },
    {
      "key": "mallen2023llm_memorization",
      "title": "When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories"
    },
    {
      "key": "chen2021codex",
      "title": "Evaluating Large Language Models Trained on Code"
    },
    {
      "key": "rein2023gpqa",
      "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
    },
    {
      "key": "phan2025hle",
      "title": "Humanity's Last Exam"
    },
    {
      "key": "aleithan2024swebenchplus",
      "title": "SWE-Bench+"
    },
    {
      "key": "jain2024livecodebench",
      "title": "LiveCodeBench"
    },
    {
      "key": "schulhoff2024prompt",
      "title": "The prompt report: A systematic survey of prompting techniques"
    },
    {
      "key": "achiam2023gpt",
      "title": "Gpt-4 technical report"
    },
    {
      "key": "gpt-neox-20b",
      "title": "GPT-NeoX-20B"
    },
    {
      "key": "zhang2018study",
      "title": "A study on overfitting in deep reinforcement learning"
    },
    {
      "key": "inan2023llama",
      "title": "Llama guard: Llm-based input-output safeguard for human-ai conversations"
    },
    {
      "key": "qwen",
      "title": "Qwen Technical Report"
    },
    {
      "key": "maiya2025open",
      "title": "Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI"
    },
    {
      "key": "turner2023activation",
      "title": "Activation addition: Steering language models without optimization"
    }
  ],
  "not_found": [
    {
      "key": "lambert2024t",
      "title": "Tbackslash"
    },
    {
      "key": "seed2025seed",
      "title": "Seed-thinking-v1. 5: Advancing superb reasoning models with reinforcement learning"
    },
    {
      "key": "kirk2024prism",
      "title": "The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models"
    },
    {
      "key": "hong2024reference",
      "title": "Reference-free monolithic preference optimization with odds ratio"
    },
    {
      "key": "wang2025nemotron",
      "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models"
    },
    {
      "key": "mlcteam2025longcat",
      "title": "Introducing LongCat-Flash-Thinking: A Technical Report"
    },
    {
      "key": "liu2025k2",
      "title": "K2-V2: A 360-Open, Reasoning-Enhanced LLM"
    },
    {
      "key": "deepseekai2025v32",
      "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models"
    },
    {
      "key": "liu2023don",
      "title": "Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding"
    },
    {
      "key": "yao2024taubench",
      "title": "tau-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains"
    },
    {
      "key": "bran2023chemcrow",
      "title": "Chemcrow: Augmenting large-language models with chemistry tools"
    },
    {
      "key": "zhang2024codeagent",
      "title": "Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges"
    },
    {
      "key": "hsieh2023distilling",
      "title": "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes"
    }
  ]
}