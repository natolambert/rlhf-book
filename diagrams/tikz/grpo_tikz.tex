% GRPO (Group Relative Policy Optimization) Architecture Diagram
% Shows: Policy, Reference, Reward models with group-based advantage normalization
% KL penalty applied as separate loss term (dashed feedback to policy)
% No value network needed

\documentclass{article}
\usepackage[margin=0.3cm, paperwidth=25cm, paperheight=12cm]{geometry}
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta, shapes.geometric, fit, calc, backgrounds}
\pagestyle{empty}

\begin{document}
\input{styles_rlhf.tex}

\begin{tikzpicture}

% Input s_t (state/prompt)
\node[plain] (st) at (1.5, 0) {$s_t$};

% Legend (below input/policy area)
\node[font=\scriptsize\itshape] (leg-key) at (1.6, -1.4) {Key};
\node[legendbox, fill=blue!15, draw=blue!70] (leg-trained) at (2.6, -1.4) {Trained};
\node[legendbox, fill=red!12, draw=red!50] (leg-frozen) at (3.9, -1.4) {Frozen};
\node[draw=black!50, dotted, rounded corners=3pt, inner sep=6pt, fit=(leg-key)(leg-frozen)] {};

% Policy Model (trained)
\node[trained] (policy) at (3.5, 0) {Policy\\Model};

% Multiple outputs in group (G samples per prompt)
\node[plain] (a1) at (5.8, 0.95) {$a_1$};
\node[plain] (a2) at (5.8, 0) {$a_2$};
\node[font=\footnotesize] at (5.8, -0.6) {...};
\node[plain] (aG) at (5.8, -1.2) {$a_G$};

% Reference Model (frozen, top)
\node[frozen] (ref) at (8.2, 0.95) {Reference\\Model};

% Reward Model (frozen, middle) - can also be verification function
\node[frozen, text width=2.2cm] (reward) at (8.2, -0.35) {Reward Model\\{\scriptsize(or verifier)}};

% Multiple rewards
\node[plain] (r1) at (10.6, 0.95) {$r_1$};
\node[plain] (r2) at (10.6, 0) {$r_2$};
\node[font=\footnotesize] at (10.6, -0.6) {...};
\node[plain] (rG) at (10.6, -1.2) {$r_G$};

% Group normalization box with equation
\node[plainwide, minimum width=2.4cm, minimum height=1.0cm] (group) at (13.0, -0.1) {$A_i = \frac{r_i - \mu}{\sigma}$};

% Multiple advantages
\node[plain] (A1) at (15.4, 0.95) {$A_1$};
\node[plain] (A2) at (15.4, 0) {$A_2$};
\node[font=\footnotesize] at (15.4, -0.6) {...};
\node[plain] (AG) at (15.4, -1.2) {$A_G$};

% Gray background boxes for groups
\begin{scope}[on background layer]
\node[groupbox, fit=(a1)(aG)] (abox) {};
\node[groupbox, fit=(r1)(rG)] (rbox) {};
\node[groupbox, fit=(A1)(AG)] (Abox) {};
\end{scope}

% ============ ARROWS ============

% s_t -> Policy
\draw[arrow] (st) -- (policy);

% Policy -> outputs group
\draw[arrow] (policy.east) -- (abox.west);

% Outputs -> Reference Model (single arrow from group)
\draw[arrow] (abox.east) to[out=0, in=180] (ref.west);

% Outputs -> Reward Model (single arrow from group)
\draw[arrow] (abox.east) to[out=0, in=180] (reward.west);

% KL FEEDBACK: Reference -> Policy (as loss term, dashed)
% Routed from top, below the main feedback arrow
\coordinate (kl-mid) at ($(ref.north) + (-3, 0.5)$);
\draw[feedbackarrow] (ref.north) to[out=120, in=0]
    (kl-mid) to[out=180, in=45]
    (policy.north east);
\node[note] at (8.0, 2.2) {KL in loss};

% Reward Model -> rewards (single arrow)
\draw[arrow] (reward.east) -- (rbox.west);

% Rewards -> Group normalization
\draw[arrow] (rbox.east) -- (group.west);

% Group normalization -> Advantages
\draw[arrow] (group.east) -- (Abox.west);

% Hidden waypoint for feedback arrow (above reward area) - raised to make room for KL arrow
\coordinate (feedback-mid) at ($(reward) + (0, 3.5)$);

% FEEDBACK: A -> Policy (policy gradient with clipping) - made more prominent
\draw[feedbackarrow, line width=1.2pt] (Abox.north west) to[out=120, in=0]
    (feedback-mid) to[out=180, in=0]
    ($(policy.north) + (0, 1.6)$) to[out=180, in=90]
    (policy.north);

% Note: Group statistics
\node[note] at (13.0, -1.8) {$\mu, \sigma$ from group};


\end{tikzpicture}

\end{document}
