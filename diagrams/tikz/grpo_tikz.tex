% GRPO (Group Relative Policy Optimization) Architecture Diagram
% Shows: Policy, Reference, Reward models with group-based advantage normalization
% KL penalty applied as separate loss term (dashed feedback to policy)
% No value network needed
% Requires styles_rlhf.tex to be loaded first

\begin{tikzpicture}

% Section label
\node[seclabel] at (0, 0) {GRPO};

% Input q (query/prompt)
\node[plain] (q) at (1.5, 0) {$q$};

% Policy Model (trained)
\node[trained] (policy) at (3.5, 0) {Policy\\Model};

% Multiple outputs in group (G samples per prompt)
\node[plain] (o1) at (5.8, 0.95) {$o_1$};
\node[plain] (o2) at (5.8, 0) {$o_2$};
\node[font=\footnotesize] at (5.8, -0.6) {...};
\node[plain] (oG) at (5.8, -1.2) {$o_G$};

% Reference Model (frozen, top)
\node[frozen] (ref) at (8.2, 0.95) {Reference\\Model};

% Reward Model (frozen, bottom)
\node[frozen] (reward) at (8.2, -1.2) {Reward\\Model};

% Multiple rewards
\node[plain] (r1) at (10.6, 0.95) {$r_1$};
\node[plain] (r2) at (10.6, 0) {$r_2$};
\node[font=\footnotesize] at (10.6, -0.6) {...};
\node[plain] (rG) at (10.6, -1.2) {$r_G$};

% Group normalization box with equation
\node[plainwide, minimum width=2.4cm, minimum height=1.0cm] (group) at (13.0, -0.1) {$A_i = \frac{r_i - \mu}{\sigma}$};

% Multiple advantages
\node[plain] (A1) at (15.4, 0.95) {$A_1$};
\node[plain] (A2) at (15.4, 0) {$A_2$};
\node[font=\footnotesize] at (15.4, -0.6) {...};
\node[plain] (AG) at (15.4, -1.2) {$A_G$};

% Gray background boxes for groups
\begin{scope}[on background layer]
\node[groupbox, fit=(o1)(oG)] (obox) {};
\node[groupbox, fit=(r1)(rG)] (rbox) {};
\node[groupbox, fit=(A1)(AG)] (Abox) {};
\end{scope}

% ============ ARROWS ============

% q -> Policy
\draw[arrow] (q) -- (policy);

% Policy -> outputs group
\draw[arrow] (policy.east) -- (obox.west);

% Three-line routing: outputs to Reference Model
\draw[arrow] (o1.east) -- (ref.west);
\draw[arrow] (o2.east) to[out=0, in=180] (ref.west);
\draw[arrow] (oG.east) to[out=30, in=180] (ref.west);

% Three-line routing: outputs to Reward Model
\draw[arrow] (o1.east) to[out=-30, in=180] (reward.west);
\draw[arrow] (o2.east) to[out=0, in=180] (reward.west);
\draw[arrow] (oG.east) -- (reward.west);

% KL FEEDBACK: Reference -> Policy (as loss term, dashed)
\draw[feedbackarrow] (ref.north) to[out=90, in=45]
    node[pos=0.3, above, note] {$KL$}
    (policy.north east);

% Reference -> rewards (for KL computation per sample)
\draw[arrow] (ref.east) -- (r1.west);
\draw[arrow] (ref.east) to[out=0, in=180] (r2.west);
\draw[arrow] (ref.east) to[out=-30, in=180] (rG.west);

% Reward Model -> rewards
\draw[arrow] (reward.east) to[out=30, in=180] (r1.west);
\draw[arrow] (reward.east) to[out=0, in=180] (r2.west);
\draw[arrow] (reward.east) -- (rG.west);

% Rewards -> Group normalization
\draw[arrow] (rbox.east) -- (group.west);

% Group normalization -> Advantages
\draw[arrow] (group.east) -- (Abox.west);

% FEEDBACK: A -> Policy (policy gradient with clipping) - made more prominent
\draw[feedbackarrow, line width=1.2pt] (Abox.north) to[out=90, in=0]
    ($(policy.north) + (0, 2.0)$) to[out=180, in=90]
    (policy.north);

% Note: Group statistics
\node[note] at (13.0, -2.0) {$\mu, \sigma$ from group};

% Legend (moved to bottom right)
\node[legendbox, fill=yellow!30] at (13.0, -2.8) {Trained};
\node[legendbox, fill=blue!12, draw=blue!50] at (15.0, -2.8) {Frozen};

\end{tikzpicture}
