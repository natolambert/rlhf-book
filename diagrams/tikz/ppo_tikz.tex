% PPO (Proximal Policy Optimization) Architecture Diagram
% Shows: Policy, Reference, Reward, Value models with GAE advantage estimation
% KL penalty combined with reward via oplus operator
% Requires styles_rlhf.tex to be loaded first

\begin{tikzpicture}

% Section label
\node[seclabel] at (0, 0) {PPO};

% Input q (query/prompt)
\node[plain] (q) at (1.5, 0) {$q$};

% Policy Model (trained)
\node[trained] (policy) at (3.5, 0) {Policy\\Model};

% Output o (single output)
\node[plain] (o) at (5.5, 0) {$o$};

% Reference Model (frozen, top)
\node[frozen] (ref) at (7.8, 1.2) {Reference\\Model};

% Reward Model (frozen, middle)
\node[frozen] (reward) at (7.8, 0) {Reward\\Model};

% Value Model (trained, bottom)
\node[trained] (value) at (7.8, -1.2) {Value\\Model};

% Oplus operator (combines reward - KL)
\node[operator] (oplus) at (10.0, 0.6) {$\oplus$};

% r output (shaped reward = reward - beta*KL)
\node[plain] (r) at (11.3, 0.6) {$\tilde{r}$};

% v output (value estimate)
\node[plain] (v) at (10.0, -1.2) {$v$};

% GAE box (Generalized Advantage Estimation)
\node[plainwide] (gae) at (12.8, -0.3) {GAE};

% A output (per-token advantages)
\node[plain] (A) at (14.3, -0.3) {$A_t$};

% ============ ARROWS ============

% q -> Policy
\draw[arrow] (q) -- (policy);

% Policy -> o
\draw[arrow] (policy) -- (o);

% o to all three models (three-line fan-out)
\draw[arrow] (o.north) to[out=60, in=180] (ref.west);
\draw[arrow] (o.east) -- (reward.west);
\draw[arrow] (o.south) to[out=-60, in=180] (value.west);

% Reference -> oplus (KL contribution)
\draw[arrow] (ref.east) to[out=0, in=120] (oplus.north west)
    node[pos=0.4, above, note] {$KL$};

% Reward -> oplus
\draw[arrow] (reward.east) to[out=0, in=-120] (oplus.south west);

% oplus -> r (shaped reward)
\draw[arrow] (oplus) -- (r);

% Value -> v
\draw[arrow] (value.east) -- (v.west);

% r -> GAE
\draw[arrow] (r.east) to[out=0, in=90] (gae.north);

% v -> GAE
\draw[arrow] (v.east) to[out=0, in=-90] (gae.south);

% GAE -> A
\draw[arrow] (gae) -- (A);

% FEEDBACK: A -> Policy (policy gradient update)
\draw[feedbackarrow] (A.north) to[out=100, in=0]
    ($(policy.north) + (0, 1.6)$) to[out=180, in=90]
    (policy.north);

% FEEDBACK: GAE -> Value (value network training)
\draw[feedbackarrow] (gae.south) to[out=-120, in=30] (value.north east);

% Note: shaped reward equation
\node[note] at (10.6, 1.5) {$\tilde{r} = r - \beta \cdot KL$};

% Legend (far right, vertically stacked)
\node[legendbox, fill=yellow!30] at (16.0, 0.5) {Trained};
\node[legendbox, fill=blue!12, draw=blue!50] at (16.0, -0.5) {Frozen};

\end{tikzpicture}
