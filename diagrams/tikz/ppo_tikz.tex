% PPO (Proximal Policy Optimization) Architecture Diagram
% Shows: Policy, Reference, Reward, Value models with GAE advantage estimation
% KL penalty combined with reward via oplus operator

\documentclass{article}
\usepackage[margin=0.3cm, paperwidth=25cm, paperheight=12cm]{geometry}
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta, shapes.geometric, fit, calc, backgrounds}
\pagestyle{empty}

\begin{document}
\input{styles_rlhf.tex}

\begin{tikzpicture}

% Input s_t (state/prompt)
\node[plain] (st) at (1.5, 0) {$s_t$};

% Legend (below input/policy area)
\node[font=\scriptsize\itshape] (leg-key) at (1.6, -1.4) {Key};
\node[legendbox, fill=blue!15, draw=blue!70] (leg-trained) at (2.6, -1.4) {Trained};
\node[legendbox, fill=red!12, draw=red!50] (leg-frozen) at (3.9, -1.4) {Frozen};
\node[draw=black!50, dotted, rounded corners=3pt, inner sep=6pt, fit=(leg-key)(leg-frozen)] {};

% Policy Model (trained)
\node[trained] (policy) at (3.5, 0) {Policy\\Model};

% Output a_t (action/completion)
\node[plain] (at) at (5.5, 0) {$a_t$};

% Reference Model (frozen, top)
\node[frozen] (ref) at (7.8, 1.2) {Reference\\Model};

% Reward Model (frozen, middle)
\node[frozen] (reward) at (7.8, 0) {Reward\\Model};

% Value Model (trained, bottom)
\node[trained] (value) at (7.8, -1.2) {Value\\Model};

% Oplus operator (combines reward - KL)
\node[operator] (oplus) at (10.0, 0.6) {$\oplus$};

% r output (shaped reward = reward - beta*KL)
\node[plain] (r) at (11.3, 0.6) {$\tilde{r}$};

% v output (value estimate)
\node[plain] (v) at (10.0, -1.2) {$v$};

% GAE box (Generalized Advantage Estimation)
\node[plainwide] (gae) at (12.8, -0.3) {GAE};

% A output (per-token advantages)
\node[plain] (A) at (14.3, -0.3) {$A_t$};

% ============ ARROWS ============

% s_t -> Policy
\draw[arrow] (st) -- (policy);

% Policy -> a_t
\draw[arrow] (policy) -- (at);

% a_t to all three models (three-line fan-out)
\draw[arrow] (at.north) to[out=60, in=180] (ref.west);
\draw[arrow] (at.east) -- (reward.west);
\draw[arrow] (at.south) to[out=-60, in=180] (value.west);

% Reference -> oplus (KL contribution)
\draw[arrow] (ref.east) to[out=0, in=120] (oplus.north west);

% Reward -> oplus
\draw[arrow] (reward.east) to[out=0, in=-120] (oplus.south west);

% oplus -> r (shaped reward)
\draw[arrow] (oplus) -- (r);

% Value -> v
\draw[arrow] (value.east) -- (v.west);

% r -> GAE
\draw[arrow] (r.east) to[out=0, in=90] (gae.north);

% v -> GAE
\draw[arrow] (v.east) to[out=0, in=-90] (gae.south);

% GAE -> A
\draw[arrow] (gae) -- (A);

% FEEDBACK: A -> Policy (policy gradient update)
\draw[feedbackarrow] (A.north) to[out=100, in=0]
    ($(policy.north) + (0, 1.6)$) to[out=180, in=90]
    (policy.north);

% FEEDBACK: GAE -> Value (value network training)
\draw[feedbackarrow] (gae.south) to[out=-120, in=30] (value.north east);

% Note: shaped reward equation (above rÌƒ block)
\node[note] at (11.3, 1.1) {$\tilde{r} = r - \beta \cdot KL$};


\end{tikzpicture}

\end{document}
