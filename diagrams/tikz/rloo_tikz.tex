% RLOO (REINFORCE Leave-One-Out) Architecture Diagram
% Shows: Policy, Reference, Reward models with leave-one-out baseline
% KL penalty folded into shaped reward (like PPO)
% No value network, no clipping
% Requires styles_rlhf.tex to be loaded first

\begin{tikzpicture}

% Section label
\node[seclabel] at (0, 0) {RLOO};

% Input q (query/prompt)
\node[plain] (q) at (1.5, 0) {$q$};

% Policy Model (trained)
\node[trained] (policy) at (3.5, 0) {Policy\\Model};

% Multiple outputs in group (K samples per prompt)
\node[plain] (o1) at (5.8, 0.95) {$o_1$};
\node[plain] (o2) at (5.8, 0) {$o_2$};
\node[font=\footnotesize] at (5.8, -0.6) {...};
\node[plain] (oK) at (5.8, -1.2) {$o_K$};

% Reward Model (frozen, top)
\node[frozen] (reward) at (8.2, 0.95) {Reward\\Model};

% Reference Model (frozen, bottom)
\node[frozen] (ref) at (8.2, -1.2) {Reference\\Model};

% Shaped rewards (r̃ = r - β·KL)
\node[plain] (r1) at (11.2, 0.95) {$\tilde{r}_1$};
\node[plain] (r2) at (11.2, 0) {$\tilde{r}_2$};
\node[font=\footnotesize] at (11.2, -0.6) {...};
\node[plain] (rK) at (11.2, -1.2) {$\tilde{r}_K$};

% Leave-One-Out box with equation
\node[plainwide, minimum width=2.8cm, minimum height=1.0cm] (loo) at (13.8, -0.1) {$A_i = \tilde{r}_i - \frac{1}{K-1}\sum_{j \neq i} \tilde{r}_j$};

% Multiple advantages
\node[plain] (A1) at (16.4, 0.95) {$A_1$};
\node[plain] (A2) at (16.4, 0) {$A_2$};
\node[font=\footnotesize] at (16.4, -0.6) {...};
\node[plain] (AK) at (16.4, -1.2) {$A_K$};

% Gray background boxes for groups
\begin{scope}[on background layer]
\node[groupbox, fit=(o1)(oK)] (obox) {};
\node[groupbox, fit=(r1)(rK)] (rbox) {};
\node[groupbox, fit=(A1)(AK)] (Abox) {};
\end{scope}

% ============ ARROWS ============

% q -> Policy
\draw[arrow] (q) -- (policy);

% Policy -> outputs group
\draw[arrow] (policy.east) -- (obox.west);

% Outputs -> Reward Model (single arrow)
\draw[arrow] (obox.east) to[out=0, in=180] (reward.west);

% Outputs -> Reference Model (single arrow)
\draw[arrow] (obox.east) to[out=0, in=180] (ref.west);

% Reward Model -> shaped rewards via oplus (show conceptually)
\draw[arrow] (reward.east) to[out=0, in=90] (10.0, 0);
\draw[arrow] (10.0, 0) -- (rbox.west);

% Reference Model -> oplus (KL contribution)
\draw[arrow] (ref.east) to[out=0, in=-90] (10.0, 0)
    node[pos=0.3, below, note] {$KL$};

% Oplus operator in the middle (single, conceptual)
\node[operator] (oplus) at (10.0, 0) {$\oplus$};

% Shaped rewards -> LOO
\draw[arrow] (rbox.east) -- (loo.west);

% LOO -> Advantages
\draw[arrow] (loo.east) -- (Abox.west);

% FEEDBACK: A -> Policy (REINFORCE, no clipping) - made more prominent
\draw[feedbackarrow, line width=1.2pt] (Abox.north) to[out=90, in=0]
    ($(policy.north) + (0, 2.0)$) to[out=180, in=90]
    (policy.north);

% Note: shaped reward equation
\node[note] at (10.0, -1.8) {$\tilde{r} = r - \beta \cdot KL$};

% Legend (far right, vertically stacked)
\node[legendbox, fill=yellow!30] at (18.0, 0.5) {Trained};
\node[legendbox, fill=blue!12, draw=blue!50] at (18.0, -0.5) {Frozen};

\end{tikzpicture}
