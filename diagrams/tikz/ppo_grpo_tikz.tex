% PPO vs GRPO vs RLOO Comparison Figure
% Based on DeepSeek-Math GRPO paper Figure 1
% Include in document with: \input{diagrams/tikz/ppo_grpo_tikz.tex}
% Requires: \usepackage{tikz}
%           \usetikzlibrary{positioning, arrows.meta, shapes.geometric, fit, calc, backgrounds}

\begin{tikzpicture}[
    % Node styles
    trained/.style={
        rectangle,
        draw=black,
        fill=yellow!30,
        minimum width=1.8cm,
        minimum height=0.8cm,
        font=\footnotesize,
        rounded corners=3pt,
        align=center
    },
    frozen/.style={
        rectangle,
        draw=blue!50,
        fill=blue!12,
        minimum width=1.8cm,
        minimum height=0.8cm,
        font=\footnotesize,
        rounded corners=3pt,
        align=center
    },
    plain/.style={
        rectangle,
        draw=black!70,
        fill=white,
        minimum width=0.8cm,
        minimum height=0.6cm,
        font=\footnotesize,
        rounded corners=3pt
    },
    plainwide/.style={
        rectangle,
        draw=black!70,
        fill=white,
        minimum width=1.6cm,
        minimum height=0.8cm,
        font=\footnotesize,
        rounded corners=3pt,
        align=center
    },
    groupbox/.style={
        rectangle,
        draw=black!30,
        fill=black!5,
        rounded corners=4pt,
        inner sep=8pt
    },
    operator/.style={
        circle,
        draw=black!70,
        fill=white,
        minimum size=0.6cm,
        font=\footnotesize
    },
    seclabel/.style={
        font=\bfseries\Large
    },
    arrow/.style={
        ->,
        >=Stealth,
        thick
    },
    feedbackarrow/.style={
        ->,
        >=Stealth,
        thick,
        draw=black
    },
    legendbox/.style={
        rectangle,
        draw=black,
        minimum width=1.8cm,
        minimum height=0.8cm,
        font=\footnotesize,
        rounded corners=3pt,
        align=center
    },
    note/.style={
        font=\scriptsize\itshape,
        text=black!70
    }
]

% ============ PPO SECTION ============
\node[seclabel] at (0, 0) {PPO};

% Input q
\node[plain] (ppo-q) at (1.5, 0) {$q$};

% Policy Model
\node[trained] (ppo-policy) at (3.5, 0) {Policy\\Model};

% Output o
\node[plain] (ppo-o) at (5.5, 0) {$o$};

% Reference Model (top)
\node[frozen] (ppo-ref) at (7.8, 1.2) {Reference\\Model};

% Reward Model (middle)
\node[frozen] (ppo-reward) at (7.8, 0) {Reward\\Model};

% Value Model (bottom) - trained
\node[trained] (ppo-value) at (7.8, -1.2) {Value\\Model};

% Oplus operator
\node[operator] (ppo-oplus) at (10.0, 0.6) {$\oplus$};

% r output (shaped reward)
\node[plain] (ppo-r) at (11.3, 0.6) {$r$};

% v output
\node[plain] (ppo-v) at (10.0, -1.2) {$v$};

% GAE box
\node[plainwide] (ppo-gae) at (12.8, -0.3) {GAE};

% A output
\node[plain] (ppo-A) at (14.3, -0.3) {$A$};

% PPO Arrows
\draw[arrow] (ppo-q) -- (ppo-policy);
\draw[arrow] (ppo-policy) -- (ppo-o);

% o to all three models
\draw[arrow] (ppo-o.north) to[out=60, in=180] (ppo-ref.west);
\draw[arrow] (ppo-o.east) -- (ppo-reward.west);
\draw[arrow] (ppo-o.south) to[out=-60, in=180] (ppo-value.west);

% KL label near reference
\node[note] at (9.0, 1.5) {$KL$};

% Reference to oplus (KL contribution)
\draw[arrow] (ppo-ref.east) to[out=0, in=120] (ppo-oplus.north west);

% Reward to oplus
\draw[arrow] (ppo-reward.east) to[out=0, in=-120] (ppo-oplus.south west);

% Oplus to r
\draw[arrow] (ppo-oplus) -- (ppo-r);

% Value to v
\draw[arrow] (ppo-value.east) -- (ppo-v.west);

% r to GAE
\draw[arrow] (ppo-r.east) to[out=0, in=90] (ppo-gae.north);

% v to GAE
\draw[arrow] (ppo-v.east) to[out=0, in=-90] (ppo-gae.south);

% GAE to A
\draw[arrow] (ppo-gae) -- (ppo-A);

% FEEDBACK ARROW: A back to Policy Model
\draw[feedbackarrow] (ppo-A.north) to[out=90, in=0]
    ($(ppo-gae.north) + (0, 1.2)$) to[out=180, in=0]
    ($(ppo-o.north) + (0, 1.8)$) to[out=180, in=90]
    (ppo-policy.north);

% ============ DIVIDING LINE 1 ============
\draw[dashed, gray!50, line width=1.5pt] (-0.5, -2.5) -- (18.5, -2.5);

% ============ GRPO SECTION ============
\node[seclabel] at (0, -5.2) {GRPO};

% Input q
\node[plain] (grpo-q) at (1.5, -5.2) {$q$};

% Policy Model
\node[trained] (grpo-policy) at (3.5, -5.2) {Policy\\Model};

% Multiple outputs in group box
\node[plain] (grpo-o1) at (5.8, -4.2) {$o_1$};
\node[plain] (grpo-o2) at (5.8, -5.2) {$o_2$};
\node[font=\footnotesize] at (5.8, -5.8) {...};
\node[plain] (grpo-oG) at (5.8, -6.4) {$o_G$};

% Reference Model (top)
\node[frozen] (grpo-ref) at (8.2, -4.2) {Reference\\Model};

% Reward Model (bottom)
\node[frozen] (grpo-reward) at (8.2, -6.0) {Reward\\Model};

% Multiple rewards in group box
\node[plain] (grpo-r1) at (10.6, -4.2) {$r_1$};
\node[plain] (grpo-r2) at (10.6, -5.2) {$r_2$};
\node[font=\footnotesize] at (10.6, -5.8) {...};
\node[plain] (grpo-rG) at (10.6, -6.4) {$r_G$};

% Group Computation box
\node[plainwide, minimum width=1.8cm] (grpo-group) at (12.8, -5.3) {Group\\Computation};

% Multiple advantages in group box
\node[plain] (grpo-A1) at (15.0, -4.2) {$A_1$};
\node[plain] (grpo-A2) at (15.0, -5.2) {$A_2$};
\node[font=\footnotesize] at (15.0, -5.8) {...};
\node[plain] (grpo-AG) at (15.0, -6.4) {$A_G$};

% Gray background boxes
\begin{scope}[on background layer]
\node[groupbox, fit=(grpo-o1)(grpo-oG)] (grpo-obox) {};
\node[groupbox, fit=(grpo-r1)(grpo-rG)] (grpo-rbox) {};
\node[groupbox, fit=(grpo-A1)(grpo-AG)] (grpo-Abox) {};
\end{scope}

% GRPO Arrows
\draw[arrow] (grpo-q) -- (grpo-policy);

% Policy to outputs group
\draw[arrow] (grpo-policy.east) -- (grpo-obox.west);

% Outputs to Reference Model (top arrow)
\draw[arrow] (grpo-obox.north east) to[out=20, in=180] (grpo-ref.west);

% Outputs to Reward Model (bottom arrow)
\draw[arrow] (grpo-obox.south east) to[out=-20, in=180] (grpo-reward.west);

% KL feedback from Reference back to Policy (stays within GRPO section)
\draw[feedbackarrow] (grpo-ref.west) to[out=150, in=30]
    node[pos=0.5, above, note] {$KL$}
    (grpo-policy.north east);

% Reference to rewards (top)
\draw[arrow] (grpo-ref.east) -- (grpo-rbox.north west);

% Reward Model to rewards (bottom)
\draw[arrow] (grpo-reward.east) -- (grpo-rbox.south west);

% Rewards to Group Computation
\draw[arrow] (grpo-rbox.east) -- (grpo-group.west);

% Group Computation to advantages
\draw[arrow] (grpo-group.east) -- (grpo-Abox.west);

% FEEDBACK ARROW: A back to Policy Model
\draw[feedbackarrow] (grpo-Abox.north) to[out=90, in=0]
    ($(grpo-group.north) + (0, 0.6)$) to[out=180, in=0]
    ($(grpo-obox.north) + (0, 0.5)$) to[out=180, in=45]
    (grpo-policy.north);

% ============ DIVIDING LINE 2 ============
\draw[dashed, gray!50, line width=1.5pt] (-0.5, -7.5) -- (18.5, -7.5);

% ============ RLOO SECTION ============
\node[seclabel] at (0, -9.9) {RLOO};

% Input q
\node[plain] (rloo-q) at (1.5, -9.9) {$q$};

% Policy Model
\node[trained] (rloo-policy) at (3.5, -9.9) {Policy\\Model};

% Multiple outputs in group box
\node[plain] (rloo-o1) at (5.8, -8.9) {$o_1$};
\node[plain] (rloo-o2) at (5.8, -9.9) {$o_2$};
\node[font=\footnotesize] at (5.8, -10.5) {...};
\node[plain] (rloo-oK) at (5.8, -11.1) {$o_K$};

% Reward Model (main) - centered
\node[frozen] (rloo-reward) at (8.2, -9.9) {Reward\\Model};

% Reference Model (for KL in reward)
\node[frozen, minimum width=1.6cm, minimum height=0.7cm, font=\scriptsize] (rloo-ref) at (8.2, -11.3) {Reference\\(for KL)};

% Multiple rewards in group box - note these include KL penalty
\node[plain] (rloo-r1) at (10.6, -8.9) {$\tilde{r}_1$};
\node[plain] (rloo-r2) at (10.6, -9.9) {$\tilde{r}_2$};
\node[font=\footnotesize] at (10.6, -10.5) {...};
\node[plain] (rloo-rK) at (10.6, -11.1) {$\tilde{r}_K$};

% Leave-One-Out box
\node[plainwide, minimum width=1.8cm] (rloo-loo) at (12.8, -10.0) {Leave-One-\\Out};

% Multiple advantages in group box
\node[plain] (rloo-A1) at (15.0, -8.9) {$A_1$};
\node[plain] (rloo-A2) at (15.0, -9.9) {$A_2$};
\node[font=\footnotesize] at (15.0, -10.5) {...};
\node[plain] (rloo-AK) at (15.0, -11.1) {$A_K$};

% Gray background boxes
\begin{scope}[on background layer]
\node[groupbox, fit=(rloo-o1)(rloo-oK)] (rloo-obox) {};
\node[groupbox, fit=(rloo-r1)(rloo-rK)] (rloo-rbox) {};
\node[groupbox, fit=(rloo-A1)(rloo-AK)] (rloo-Abox) {};
\end{scope}

% RLOO Arrows
\draw[arrow] (rloo-q) -- (rloo-policy);

% Policy to outputs group
\draw[arrow] (rloo-policy.east) -- (rloo-obox.west);

% Outputs to Reward Model
\draw[arrow] (rloo-obox.east) -- (rloo-reward.west);

% Outputs to Reference (for KL computation)
\draw[arrow, black!60] (rloo-obox.south east) to[out=-20, in=180] (rloo-ref.west);

% Reward Model to rewards group
\draw[arrow] (rloo-reward.east) -- (rloo-rbox.west);

% Reference contributes KL penalty to rewards
\draw[arrow, black!60] (rloo-ref.east) to[out=0, in=-90] (rloo-rbox.south);

% Note explaining shaped reward
\node[note, anchor=west] at (9.2, -11.8) {$\tilde{r} = r - \beta \cdot KL$};

% Rewards to LOO computation
\draw[arrow] (rloo-rbox.east) -- (rloo-loo.west);

% LOO to advantages
\draw[arrow] (rloo-loo.east) -- (rloo-Abox.west);

% FEEDBACK ARROW: A back to Policy Model (no clipping, direct policy gradient)
\draw[feedbackarrow] (rloo-Abox.north) to[out=90, in=0]
    ($(rloo-loo.north) + (0, 0.5)$) to[out=180, in=0]
    ($(rloo-obox.north) + (0, 0.5)$) to[out=180, in=45]
    (rloo-policy.north);

% ============ LEGEND ============
\node[legendbox, fill=yellow!30] at (17.5, 0) {Trained\\Models};
\node[legendbox, fill=blue!12, draw=blue!50] at (17.5, -1.2) {Frozen\\Models};

\end{tikzpicture}
