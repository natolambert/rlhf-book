% PPO vs GRPO Comparison Figure
% Include in document with: \input{diagrams/tikz/ppo_grpo_tikz.tex}
% Requires: \usepackage{tikz}
%           \usetikzlibrary{positioning, arrows.meta, shapes.geometric, fit, calc, backgrounds}

\begin{tikzpicture}[
    % Node styles
    trained/.style={
        rectangle,
        draw=black,
        fill=yellow!30,
        minimum width=1.5cm,
        minimum height=0.75cm,
        font=\footnotesize,
        rounded corners=3pt,
        align=center
    },
    frozen/.style={
        rectangle,
        draw=blue!50,
        fill=blue!12,
        minimum width=1.5cm,
        minimum height=0.75cm,
        font=\footnotesize,
        rounded corners=3pt,
        align=center
    },
    plain/.style={
        rectangle,
        draw=black!70,
        fill=white,
        minimum width=0.7cm,
        minimum height=0.6cm,
        font=\footnotesize,
        rounded corners=3pt
    },
    plainwide/.style={
        rectangle,
        draw=black!70,
        fill=white,
        minimum width=1.3cm,
        minimum height=0.6cm,
        font=\footnotesize,
        rounded corners=3pt,
        align=center
    },
    groupbox/.style={
        rectangle,
        draw=none,
        fill=black!6,
        rounded corners=4pt,
        inner sep=5pt
    },
    operator/.style={
        circle,
        draw=black!70,
        fill=white,
        minimum size=0.5cm,
        font=\footnotesize
    },
    seclabel/.style={
        font=\bfseries\large
    },
    arrow/.style={
        ->,
        >=Stealth,
        thick
    },
    feedbackarrow/.style={
        ->,
        >=Stealth,
        thick,
        draw=black
    },
    legendbox/.style={
        rectangle,
        draw=black,
        minimum width=1.6cm,
        minimum height=0.7cm,
        font=\footnotesize,
        rounded corners=3pt,
        align=center
    }
]

% ============ PPO SECTION ============
\node[seclabel] at (0, 0) {PPO};

% Input q
\node[plain] (ppo-q) at (1.4, 0) {$q$};

% Policy Model
\node[trained, minimum width=1.6cm] (ppo-policy) at (3.2, 0) {Policy\\Model};

% Output o
\node[plain] (ppo-o) at (5.0, 0) {$o$};

% Reference Model (top)
\node[frozen, minimum width=1.6cm] (ppo-ref) at (7.2, 1.2) {Reference\\Model};

% Reward Model (middle)
\node[frozen, minimum width=1.6cm] (ppo-reward) at (7.2, 0) {Reward\\Model};

% Value Model (bottom) - trained
\node[trained, minimum width=1.6cm] (ppo-value) at (7.2, -1.2) {Value\\Model};

% Oplus operator
\node[operator] (ppo-oplus) at (9.2, 0.6) {$\oplus$};

% r output
\node[plain] (ppo-r) at (10.3, 0.6) {$r$};

% v output
\node[plain] (ppo-v) at (9.2, -1.2) {$v$};

% GAE box
\node[plainwide] (ppo-gae) at (11.5, -0.3) {GAE};

% A output
\node[trained, minimum width=0.8cm] (ppo-A) at (13.0, -0.3) {$A$};

% PPO Arrows
\draw[arrow] (ppo-q) -- (ppo-policy);
\draw[arrow] (ppo-policy) -- (ppo-o);

% Curved arrow from o to reference model
\draw[arrow] (ppo-o.north) to[out=50, in=180] (ppo-ref.west);
% Straight arrow from o to reward model
\draw[arrow] (ppo-o.east) -- (ppo-reward.west);
% Curved arrow from o to value model
\draw[arrow] (ppo-o.south) to[out=-50, in=180] (ppo-value.west);

% KL label
\node[font=\footnotesize\itshape] at (8.2, 1.6) {$KL$};

% Reference to oplus
\draw[arrow] (ppo-ref.east) to[out=0, in=100] (ppo-oplus.north);

% Reward to oplus
\draw[arrow] (ppo-reward.east) to[out=0, in=-100] (ppo-oplus.south);

% Oplus to r
\draw[arrow] (ppo-oplus) -- (ppo-r);

% Value to v
\draw[arrow] (ppo-value.east) -- (ppo-v.west);

% r to GAE
\draw[arrow] (ppo-r.east) to[out=0, in=90] (ppo-gae.north);

% v to GAE
\draw[arrow] (ppo-v.east) to[out=0, in=-90] (ppo-gae.south);

% GAE to A
\draw[arrow] (ppo-gae) -- (ppo-A);

% GAE feedback to Value Model (dashed - training signal)
\draw[feedbackarrow, dashed] (ppo-gae.south) to[out=-120, in=0] (ppo-value.east);

% FEEDBACK ARROW: A back to Policy Model
\draw[feedbackarrow] (ppo-A.north) to[out=90, in=0]
    ($(ppo-gae.north) + (0, 1.5)$) to[out=180, in=0]
    ($(ppo-o.north) + (0, 2.0)$) to[out=180, in=90]
    (ppo-policy.north);

% ============ DIVIDING LINE ============
\draw[dashed, gray!50, line width=1.5pt] (-0.3, -2.4) -- (15.0, -2.4);

% ============ GRPO SECTION ============
% Shifted down to avoid KL arrow crossing divider
\node[seclabel] at (0, -4.5) {GRPO};

% Input q
\node[plain] (grpo-q) at (1.4, -4.5) {$q$};

% Policy Model
\node[trained, minimum width=1.6cm] (grpo-policy) at (3.2, -4.5) {Policy\\Model};

% Multiple outputs o1, o2, ..., oG
\node[plain] (grpo-o1) at (5.2, -3.6) {$o_1$};
\node[plain] (grpo-o2) at (5.2, -4.4) {$o_2$};
\node[font=\footnotesize] (grpo-odots) at (5.2, -5.0) {...};
\node[plain] (grpo-oG) at (5.2, -5.6) {$o_G$};

% Reference Model
\node[frozen, minimum width=1.6cm] (grpo-ref) at (7.2, -3.6) {Reference\\Model};

% Reward Model
\node[frozen, minimum width=1.6cm] (grpo-reward) at (7.2, -5.1) {Reward\\Model};

% Multiple rewards r1, r2, ..., rG
\node[plain] (grpo-r1) at (9.4, -3.6) {$r_1$};
\node[plain] (grpo-r2) at (9.4, -4.4) {$r_2$};
\node[font=\footnotesize] (grpo-rdots) at (9.4, -5.0) {...};
\node[plain] (grpo-rG) at (9.4, -5.6) {$r_G$};

% Group Computation box
\node[plainwide, minimum width=1.7cm, minimum height=0.8cm] (grpo-group) at (11.3, -4.6) {Group\\Computation};

% Multiple advantages A1, A2, ..., AG
\node[plain] (grpo-A1) at (13.2, -3.6) {$A_1$};
\node[plain] (grpo-A2) at (13.2, -4.4) {$A_2$};
\node[font=\footnotesize] (grpo-Adots) at (13.2, -5.0) {...};
\node[plain] (grpo-AG) at (13.2, -5.6) {$A_G$};

% Gray background boxes
\begin{scope}[on background layer]
\node[groupbox, fit=(grpo-o1)(grpo-oG), inner sep=6pt] {};
\node[groupbox, fit=(grpo-r1)(grpo-rG), inner sep=6pt] {};
\node[groupbox, fit=(grpo-A1)(grpo-AG), inner sep=6pt] {};
\end{scope}

% GRPO Arrows
\draw[arrow] (grpo-q) -- (grpo-policy);

% Policy to outputs
\draw[arrow] (grpo-policy.east) to[out=20, in=180] (grpo-o1.west);
\draw[arrow] (grpo-policy.east) to[out=5, in=180] (grpo-o2.west);
\draw[arrow] (grpo-policy.east) to[out=-20, in=180] (grpo-oG.west);

% ALL Outputs to Reference Model (all o's connect)
\draw[arrow] (grpo-o1.east) -- (grpo-ref.west);
\draw[arrow] (grpo-o2.east) to[out=0, in=180] ($(grpo-ref.west) + (0, -0.15)$);
\draw[arrow] (grpo-oG.east) to[out=0, in=200] (grpo-ref.south west);

% ALL Outputs to Reward Model (all o's connect)
\draw[arrow] (grpo-o1.east) to[out=0, in=160] (grpo-reward.north west);
\draw[arrow] (grpo-o2.east) to[out=0, in=180] (grpo-reward.west);
\draw[arrow] (grpo-oG.east) to[out=0, in=200] (grpo-reward.south west);

% KL label with arc from Reference back toward Policy
\node[font=\footnotesize\itshape] at (6.2, -2.9) {$KL$};
\draw[feedbackarrow] (grpo-ref.north) to[out=100, in=0] (5.8, -2.8) to[out=180, in=60] (grpo-policy.north east);

% Reference contributes to r (KL penalty added)
\draw[arrow] (grpo-ref.east) -- (grpo-r1.west);

% Reward to rewards
\draw[arrow] (grpo-reward.east) to[out=20, in=180] (grpo-r2.west);
\draw[arrow] (grpo-reward.east) to[out=-10, in=180] (grpo-rG.west);

% Rewards to group computation
\draw[arrow] (grpo-r1.east) to[out=0, in=130] (grpo-group.north west);
\draw[arrow] (grpo-r2.east) to[out=0, in=180] (grpo-group.west);
\draw[arrow] (grpo-rG.east) to[out=0, in=-130] (grpo-group.south west);

% Group computation to advantages
\draw[arrow] (grpo-group.east) to[out=30, in=180] (grpo-A1.west);
\draw[arrow] (grpo-group.east) to[out=10, in=180] (grpo-A2.west);
\draw[arrow] (grpo-group.east) to[out=-30, in=180] (grpo-AG.west);

% FEEDBACK ARROW: A back to Policy Model (routes alongside KL arrow)
\draw[feedbackarrow] (grpo-A1.west) to[out=180, in=0]
    ($(grpo-r1.north) + (0, 0.5)$) to[out=180, in=0]
    ($(grpo-ref.north) + (0.8, 0.6)$) to[out=180, in=30]
    (grpo-policy.east);

% ============ LEGEND ============
\node[legendbox, fill=yellow!30] (legend-trained) at (15.2, 0) {Trained\\Models};
\node[legendbox, fill=blue!12, draw=blue!50] (legend-frozen) at (15.2, -1.2) {Frozen\\Models};

\end{tikzpicture}
