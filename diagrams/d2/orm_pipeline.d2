# Outcome Reward Model (ORM) Pipeline
# Data → Model → Output → Loss

direction: right

title: "Outcome Reward Model (ORM)" {
  shape: text
  style.font-size: 24
  style.bold: true
}

# Data inputs
data: "Data" {
  shape: rectangle
  style.fill: "#e8f4f8"

  prompt: "prompt x"
  completion: "completion y"
  label: "label: correct/incorrect"
}

# Model
model: "Model" {
  shape: rectangle
  style.fill: "#f0f0f0"

  trunk: "LM Trunk" {
    shape: rectangle
  }
  head: "Per-Token\nClassifier" {
    shape: rectangle
    style.fill: "#fff3cd"
  }

  trunk -> head
}

# Outputs
outputs: "Scores" {
  shape: rectangle
  style.fill: "#e8f4f8"

  probs: "p(correct | t)\nfor each token t"
}

# Loss
loss: "Loss" {
  shape: rectangle
  style.fill: "#f8d7da"

  formula: "L = Σ BCE(p_t, label)\nprompt masked" {
    shape: text
    style.font-size: 14
  }
}

# Connections
data -> model: "tokenize"
model -> outputs
outputs -> loss

# Annotation
note: "Per-token correctness\nPrompt tokens masked\nCompletion supervised" {
  shape: text
  style.font-size: 12
  style.italic: true
}
