# SimPO Configuration
# Simple Preference Optimization (Meng et al., 2024)
# Paper: https://arxiv.org/abs/2405.14734
#
# SimPO removes the reference model and uses length-normalized log probs.
# Key changes: average log prob instead of sum, no reference model.
#
# Run with:
#   uv run python -m direct_alignment.train --config configs/simpo.yaml

# Model
model_name: allenai/OLMo-2-0425-1B-SFT

# Algorithm
loss: simpo
beta: 2.0   # SimPO typically uses higher beta (2.0-2.5)
gamma: 0.5  # Gamma/beta ratio (official SimPO notation)

# Dataset
dataset_name: argilla/ultrafeedback-binarized-preferences-cleaned
dataset_split: train
max_samples: 12800  # 12800 / 64 batch * 3 epochs = 600 steps
max_length: 2048

# Training
learning_rate: 8.0e-7  # Slightly lower than prior run for extra stability
num_epochs: 3  # ~600 optimizer steps total
batch_size: 8
gradient_accumulation_steps: 8  # Effective batch size = 64
warmup_ratio: 0.1
max_grad_norm: 1.0

# Hardware
gradient_checkpointing: true
bf16: true

# Logging
wandb_project: null
log_every: 10
sample_every: 50
sample_num_prompts: 4
sample_prompt_strategy: round_robin

# Output
output_dir: ./outputs
save_model: false

seed: 42
