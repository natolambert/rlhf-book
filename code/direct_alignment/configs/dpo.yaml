# DPO Configuration
# Direct Preference Optimization (Rafailov et al., 2023)
# Paper: https://arxiv.org/abs/2305.18290
#
# Run with:
#   uv run python -m direct_alignment.train --config configs/dpo.yaml

# Model
model_name: allenai/OLMo-2-0425-1B-SFT

# Algorithm
loss: dpo
beta: 0.1  # KL penalty - lower = more regularization

# Dataset
dataset_name: argilla/ultrafeedback-binarized-preferences-cleaned
dataset_split: train
max_samples: 1000
max_length: 512

# Training
learning_rate: 5e-7  # DPO requires very low learning rate
num_epochs: 1
batch_size: 4
gradient_accumulation_steps: 4
warmup_ratio: 0.1
max_grad_norm: 1.0

# Hardware
gradient_checkpointing: true
bf16: true

# Logging
wandb_project: null  # Set via WANDB_PROJECT env var or change here
log_every: 10

# Output
output_dir: ./outputs
save_model: false

seed: 42
